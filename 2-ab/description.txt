The goal of this practical work is to build a simple neural network and to become familiar with these models
and how to train them with the backpropagation (or backpropagation) of the gradient.
To do this, we will begin by theoretically studying a perceptron with a hidden layer and its learning
procedure. We will then implement this network with the PyTorch library first on a toy problem to check
that it is working correctly, then on the MNIST dataset.
The site http://playground.tensorflow.org makes it possible to visualize the functioning and the
learning of small neural networks
